{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 - Alternative Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Bridging Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sections we gave a deep exploration the changes in performance of a MLP on the KDD under the action of three different optimisers: 'adam', 'NAG' and 'SGD'. Alas, in more typical cyber settings we could only dream of having such a clean dataset, and this is where we find motivation for the coming section. When faced with the challenge of trying to find a more typical cyber datasets we were forced to abstract slightly- instead of asking if we could find a realistic cyber dataset, we asked what structures cyber data often fall into, and concluded that sequence (or time series) data, and graphs were common structures. We eventually found a dataset which could kill two birds with one stone, a dataset of malware (and non-malware) API call sequences [1]. From this we could just use the sequence data immediately, as well as trivially generate graphs for each sequence, where we connected API call `a` and API call `b` if and only if `b` was directly after `a` at some point in the sequence.\n",
    "\n",
    "We start by reading in the API call dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>071e8c3f8922e186e57548cd4c703a5d</td>\n",
       "      <td>112</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>274</td>\n",
       "      <td>158</td>\n",
       "      <td>215</td>\n",
       "      <td>298</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>208</td>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>71</td>\n",
       "      <td>297</td>\n",
       "      <td>135</td>\n",
       "      <td>171</td>\n",
       "      <td>215</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b68abd064e975e1c6d5f25e748663076</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>112</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>113</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72049be7bd30ea61297ea624ae198067</td>\n",
       "      <td>82</td>\n",
       "      <td>208</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>117</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>302</td>\n",
       "      <td>228</td>\n",
       "      <td>302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c9b3700a77facf29172f32df6bc77f48</td>\n",
       "      <td>82</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>240</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>40</td>\n",
       "      <td>209</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>141</td>\n",
       "      <td>260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               hash  t_0  t_1  t_2  t_3  t_4  t_5  t_6  t_7  \\\n",
       "0  071e8c3f8922e186e57548cd4c703a5d  112  274  158  215  274  158  215  298   \n",
       "1  33f8e6d08a6aae939f25a8e0d63dd523   82  208  187  208  172  117  172  117   \n",
       "2  b68abd064e975e1c6d5f25e748663076   16  110  240  117  240  117  240  117   \n",
       "3  72049be7bd30ea61297ea624ae198067   82  208  187  208  172  117  172  117   \n",
       "4  c9b3700a77facf29172f32df6bc77f48   82  240  117  240  117  240  117  240   \n",
       "\n",
       "   t_8  ...  t_91  t_92  t_93  t_94  t_95  t_96  t_97  t_98  t_99  malware  \n",
       "0   76  ...    71   297   135   171   215    35   208    56    71        1  \n",
       "1  172  ...    81   240   117    71   297   135   171   215    35        1  \n",
       "2  240  ...    65   112   123    65   112   123    65   113   112        1  \n",
       "3  172  ...   208   302   208   302   187   208   302   228   302        1  \n",
       "4  117  ...   209   260    40   209   260   141   260   141   260        1  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"API_Calls.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first 100 calls are given in sequence for each program, and the final column gives us a binary label, whether or not the sequence is malware. We also note that each different kind of API call is given by an integer. It's clear that we don't need to worry about the `hash` column, and immediately drop this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billnunn/opt/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('hash', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think it's worth noting that the reader may correctly identify that our API call dataset is at least as idealised and unrealistic as the KDD data. To this we say that this section isn't really about the classification performance: the API data is instead used as a 'toy' example to explore neural architectures more appropriate for realistic cyber settings. The first architecture we try out is in **Section 4.3**, we construct a recurrent neural network (RNN) on the API calls treated as sequence data. After this we then discuss convolutional networks in **Section 4.4**, paying special attention to the case where our input data is graphs, as is the case for our API call graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - RNN Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is always the case, we do some quick preprocessing before we can get on with our RNNs in **Section 4.3**. We first decide upon what proportion of the data we want to use for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.7\n",
    "cut = round(df.shape[0]*0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we shuffle `df` and split into the training and test sets. We also carry out a quick sanity check to see that the full data has been partitioned into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1)\n",
    "train_df = df[0:cut]\n",
    "test_df = df[cut:df.shape[0]+1]\n",
    "train_df.shape[0] + test_df.shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to convert our dataframes into `numpy` arrays, ready for input into a `keras` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billnunn/opt/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/billnunn/opt/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_df.drop('malware', 1))\n",
    "X_test = np.array(test_df.drop('malware', 1))\n",
    "\n",
    "y_train = np.array(train_df['malware'])\n",
    "y_test = np.array(test_df['malware'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are good to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encountered the underpinning mathematical ideas of recurrent neural networks in the DST lectures. We therefore refrain from discussing the mathematical background of this architecture (luckily/unluckily we are barely in control of our mathematical spewings and some maths will undoubtedly emerge in **Section 4.4**). As with all our previous networks we start by defining the layers of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Input layer, note input dimension is dictionary size + 1:\n",
    "model.add(layers.Embedding(input_dim=307, output_dim=20))\n",
    "\n",
    "# RNN layer with 12 internal units:\n",
    "model.add(layers.LSTM(12))\n",
    "\n",
    "# Dense layer with 12 units:\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "\n",
    "# Bianry classification layer, sigmoid to emulate a probability:\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to summarise this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 20)          6140      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 12)                1584      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 7,893\n",
      "Trainable params: 7,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plan from here is to continue the analysis of the earlier parts of the report, and compile using a range of optimisers and then fit these models to our sequence data. When we compile we ensure we use binary cross entropy as our loss function- remember we are only doing binary classification with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before compiling our model we note that the dataset has highly imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of malware sequences: 42797 . Number of non-malware sequences: 1079\n"
     ]
    }
   ],
   "source": [
    "mal = 0\n",
    "good = 0\n",
    "for i in df['malware']:\n",
    "    if i==0:\n",
    "        good = good + 1\n",
    "    else:\n",
    "        mal = mal + 1\n",
    "\n",
    "print(\"Number of malware sequences:\", mal, \n",
    "      \". Number of non-malware sequences:\", good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we shall see, the class imbalance is a real issue- our go to loss functions (binary cross entropy and friends) may not be the most sensible thing to want to minimse. We found a neat blog post [5] and we would have given this a try if we had the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 - Adam Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=[tf.keras.metrics.TrueNegatives(),\n",
    "                       tf.keras.metrics.TruePositives()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the imbalanced classes, we use the number of true negatives and true positives as our metrics, and compare to the number in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training non-malware sequences: 740 . Number of training malware sequences: 29973\n"
     ]
    }
   ],
   "source": [
    "trainmal = 0\n",
    "traingood = 0\n",
    "for i in train_df['malware']:\n",
    "    if i==0:\n",
    "        traingood = traingood + 1\n",
    "    else:\n",
    "        trainmal = trainmal + 1\n",
    "print(\"Number of training non-malware sequences:\", traingood, \n",
    "      \". Number of training malware sequences:\", trainmal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above numbers in mind we train our network with the adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 0.1425 - true_negatives: 0.0000e+00 - true_positives: 29973.0000\n",
      "Epoch 2/10\n",
      "308/308 [==============================] - 49s 159ms/step - loss: 0.0779 - true_negatives: 0.0000e+00 - true_positives: 29973.00005s - loss: 0.0786 - true_negatives: 0.0\n",
      "Epoch 3/10\n",
      "308/308 [==============================] - 53s 171ms/step - loss: 0.0676 - true_negatives: 0.0000e+00 - true_positives: 29973.0000\n",
      "Epoch 4/10\n",
      "308/308 [==============================] - 52s 169ms/step - loss: 0.0604 - true_negatives: 1.0000 - true_positives: 29973.0000\n",
      "Epoch 5/10\n",
      "308/308 [==============================] - 52s 169ms/step - loss: 0.0562 - true_negatives: 150.0000 - true_positives: 29954.0000\n",
      "Epoch 6/10\n",
      "308/308 [==============================] - 55s 178ms/step - loss: 0.0528 - true_negatives: 271.0000 - true_positives: 29921.00002s - loss: 0.0529 - true_negatives: 253.0000 - true_posi\n",
      "Epoch 7/10\n",
      "308/308 [==============================] - 49s 160ms/step - loss: 0.0506 - true_negatives: 409.0000 - true_positives: 29833.0000\n",
      "Epoch 8/10\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 0.0465 - true_negatives: 427.0000 - true_positives: 29827.0000\n",
      "Epoch 9/10\n",
      "308/308 [==============================] - 61s 200ms/step - loss: 0.0440 - true_negatives: 461.0000 - true_positives: 29821.0000\n",
      "Epoch 10/10\n",
      "308/308 [==============================] - 55s 179ms/step - loss: 0.0412 - true_negatives: 468.0000 - true_positives: 29843.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc880f82290>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate our network on the test set. Again the true negative and true positive counts are what we're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing non-malware sequences: 339 . Number of testing malware sequences: 12824\n"
     ]
    }
   ],
   "source": [
    "testmal = 0\n",
    "testgood = 0\n",
    "for i in test_df['malware']:\n",
    "    if i==0:\n",
    "        testgood = testgood + 1\n",
    "    else:\n",
    "        testmal = testmal + 1\n",
    "print(\"Number of testing non-malware sequences:\", testgood, \n",
    "      \". Number of testing malware sequences:\", testmal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 12s 28ms/step - loss: 0.0560 - true_negatives: 174.0000 - true_positives: 12798.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.055981628596782684, 174.0, 12798.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss on the test set holds up well to the training loss, and it is therefore unlikely that we've overfitted. As was forewarned we have a greatly reduced accuracy on the smaller class, we've done significant playing behind the scenes with continued use of binary cross entropy as our loss but can't seem to resolve this. When we begin to capture a significantly better proportion of true negatives we have already begun an inescapable flirtation with overfitting. To combat this we could try adding drop out, or the alternative loss function suggested in [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 - NAG Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pinch Mo's code for defining a NAG optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nag_opt = tf.keras.optimizers.SGD(momentum = 0.9, nesterov = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we repeat the previous section exchanging `adam` for `nag_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Input layer, note input dimension is dictionary size + 1:\n",
    "model.add(layers.Embedding(input_dim=307, output_dim=20))\n",
    "\n",
    "# RNN layer with 12 internal units:\n",
    "model.add(layers.LSTM(12))\n",
    "\n",
    "# Dense layer with 12 units:\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "\n",
    "# Bianry classification layer, sigmoid to emulate a probability:\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=nag_opt, \n",
    "              metrics=[tf.keras.metrics.TrueNegatives(),\n",
    "                       tf.keras.metrics.TruePositives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "308/308 [==============================] - 54s 175ms/step - loss: 0.1399 - true_negatives_1: 1.0000 - true_positives_1: 29965.0000\n",
      "Epoch 2/10\n",
      "308/308 [==============================] - 56s 182ms/step - loss: 0.1132 - true_negatives_1: 0.0000e+00 - true_positives_1: 29973.0000\n",
      "Epoch 3/10\n",
      "308/308 [==============================] - 58s 187ms/step - loss: 0.1128 - true_negatives_1: 0.0000e+00 - true_positives_1: 29973.0000\n",
      "Epoch 4/10\n",
      "308/308 [==============================] - 50s 163ms/step - loss: 0.1123 - true_negatives_1: 0.0000e+00 - true_positives_1: 29973.0000\n",
      "Epoch 5/10\n",
      "308/308 [==============================] - 48s 156ms/step - loss: 0.1112 - true_negatives_1: 0.0000e+00 - true_positives_1: 29973.0000\n",
      "Epoch 6/10\n",
      "308/308 [==============================] - 49s 160ms/step - loss: 0.1081 - true_negatives_1: 0.0000e+00 - true_positives_1: 29973.0000\n",
      "Epoch 7/10\n",
      "308/308 [==============================] - 52s 167ms/step - loss: 0.1000 - true_negatives_1: 0.0000e+00 - true_positives_1: 29973.0000\n",
      "Epoch 8/10\n",
      "308/308 [==============================] - 51s 167ms/step - loss: 0.0945 - true_negatives_1: 0.0000e+00 - true_positives_1: 29973.0000\n",
      "Epoch 9/10\n",
      "308/308 [==============================] - 52s 169ms/step - loss: 0.0896 - true_negatives_1: 0.0000e+00 - true_positives_1: 29973.0000\n",
      "Epoch 10/10\n",
      "308/308 [==============================] - 44s 143ms/step - loss: 0.0853 - true_negatives_1: 0.0000e+00 - true_positives_1: 29973.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc887e74c10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 12s 28ms/step - loss: 0.0912 - true_negatives_1: 0.0000e+00 - true_positives_1: 12824.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09121925383806229, 0.0, 12824.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, the outcome of the NAG is brutal (and happens repeatedly upon re-runs): every call sequence is deemed malware. Like adam did initially, NAG gets caught in a position where it predicts no true negatives on the training set, and gets stuck at the 'local minima' produced by the large class imbalance. In contrast, the adam optimiser somwhow escapes by epoch four- this really feels like black magic and we must learn out how it works in detail at a later date! Furthermore, we wonder if there is something inherent in the LSTM architecture which results in such a large disparity between adam and NAG. Recall that with the MLP architecture of the earlier sections the contrast wasn't quite so stark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 - SGD Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we thank Mo for his earlier code, by nicking his other optimiser. We don't think things bode well for SGD after NAG's poor showing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_opt = tf.keras.optimizers.SGD(momentum = 0.9, nesterov = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here you have the pleasure of more repeat code, third time's a charm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Input layer, note input dimension is dictionary size + 1:\n",
    "model.add(layers.Embedding(input_dim=307, output_dim=20))\n",
    "\n",
    "# RNN layer with 12 internal units:\n",
    "model.add(layers.LSTM(12))\n",
    "\n",
    "# Dense layer with 12 units:\n",
    "model.add(layers.Dense(12, activation='relu'))\n",
    "\n",
    "# Bianry classification layer, sigmoid to emulate a probability:\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd_opt, \n",
    "              metrics=[tf.keras.metrics.TrueNegatives(),\n",
    "                       tf.keras.metrics.TruePositives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "308/308 [==============================] - 53s 172ms/step - loss: 0.1508 - true_negatives_2: 1.0000 - true_positives_2: 29885.0000\n",
      "Epoch 2/10\n",
      "308/308 [==============================] - 51s 165ms/step - loss: 0.1133 - true_negatives_2: 0.0000e+00 - true_positives_2: 29973.0000\n",
      "Epoch 3/10\n",
      "308/308 [==============================] - 50s 163ms/step - loss: 0.1130 - true_negatives_2: 0.0000e+00 - true_positives_2: 29973.0000\n",
      "Epoch 4/10\n",
      "308/308 [==============================] - 50s 162ms/step - loss: 0.1126 - true_negatives_2: 0.0000e+00 - true_positives_2: 29973.0000\n",
      "Epoch 5/10\n",
      "308/308 [==============================] - 49s 159ms/step - loss: 0.1121 - true_negatives_2: 0.0000e+00 - true_positives_2: 29973.0000\n",
      "Epoch 6/10\n",
      "308/308 [==============================] - 50s 163ms/step - loss: 0.1114 - true_negatives_2: 0.0000e+00 - true_positives_2: 29973.0000\n",
      "Epoch 7/10\n",
      "308/308 [==============================] - 49s 158ms/step - loss: 0.1096 - true_negatives_2: 0.0000e+00 - true_positives_2: 29973.0000\n",
      "Epoch 8/10\n",
      "308/308 [==============================] - 53s 173ms/step - loss: 0.1057 - true_negatives_2: 0.0000e+00 - true_positives_2: 29973.0000\n",
      "Epoch 9/10\n",
      "308/308 [==============================] - 46s 149ms/step - loss: 0.0980 - true_negatives_2: 0.0000e+00 - true_positives_2: 29973.0000\n",
      "Epoch 10/10\n",
      "308/308 [==============================] - 44s 141ms/step - loss: 0.0923 - true_negatives_2: 0.0000e+00 - true_positives_2: 29973.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc884c28550>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412/412 [==============================] - 12s 30ms/step - loss: 0.1024 - true_negatives_2: 0.0000e+00 - true_positives_2: 12824.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10241524130105972, 0.0, 12824.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we anticipated, SGD should do no better than NAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Graph Convolution Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike recurrent neural networks, we didn't encounter graph convolution in DST lectures and so we start this section by giving some of the mathematical background on the subject. Our discussion closely follows the graph convolution method detailed in [2].\n",
    "\n",
    "The method detailed in [2] is surprising: it acts on graphs directly; the graphs arenâ€™t required to have the same number of nodes; the nodes themselves donâ€™t even need to be labeled. Furthermore, the whole network can be fitted with backpropagation alone. Earlier neural network methods for graph classification (such as PATCHY-SAN [3]) were reliant on heavy preprocessing of data, and the preprocessing could not be learned via direct backpropagation of the final output. The suggestion for why such an improbable sounding method is effective is that we implicitly apply a graph kernel similar to a Weisfeiler-Lehman kernel [4] (see Bill's reflection for discussion of WL kernels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 - Stages of the Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network splits into three main parts:\n",
    "\n",
    "* The convolution layers.\n",
    "* The sort pooling layer.\n",
    "* The remaining layers.\n",
    "\n",
    "We explain only the first two of these parts as the 'remaining layers' don't take a specific form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 - The Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $G$ be a graph with $n$ vertices. \n",
    "\n",
    "Let's suppose we choose to have $h$ convolution layers indexed by $t$, which output an $n$ rowed matrix $Z^t$, where we want the $i$ th rows of our $Z$ matrices to give information about the 'structural role' of vertex $i$. We define $Z^0$ to be a special matrix $X$ which has dimension $n$ by $c_0$. In the case where we have labeled vertices we can let $X$ be the one hot encoding of the vertices i.e. the $n$ by $n$ identity matrix. In the case where the nodes are unlabeled we can also make a sensible choice for $X$, the column vector of normalised node degrees is suggested.\n",
    "\n",
    "From here the convolution step takes the following form:\n",
    "\n",
    "$$Z^{t+1} = f(\\tilde D^{-1} \\tilde A Z^{t}W^{t})$$\n",
    "\n",
    "Where $W$ is the $c_t$ by $c_{t+1}$ matrix whose columns are the filter weights (in the usual CNN sense) for a given filter. The matrix $\\tilde A$ is the $A+I$ where $A$ is the adjacency matrix, and $\\tilde D$ is the degree matrix of the extended adjacency matrix $\\tilde A$. We note that the application of $\\tilde D^{-1}$ is to normalise the result of applying $\\tilde A$. Finally $f$ is our non-linear activation function applied to the matrix point wise. Since $Z^0$ is $n$ by $c_0$ and $W^0$ is $c_0$ by $c_1$ it follows that $Z^1$ is $n$ by $c_1$. And similarly $Z^t$ is $n$ by $c_t$ using induction. Now matrix $Z_t$ really just stores the outcome of applying $c_t$ convolutional filters on the outputs of the previous time step's filters, where only filter outputs of adjacent nodes are mixed in accordance with $\\tilde A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is usual in CNNs the outputs of the $h$ convolution layers are gathered up- we simply column bind matrices $Z^1, Z^2,..., Z^h$ to get matrix $Z^{1:h}$ which is clearly $n$ by $\\sum^{h}_{t=1} c_t$. Each row of $Z^{1:h}$ contains deep structural information about its corresponding node, and hopefully useful information once we've trained the filter weights! The following lemma (of ours) illustrates this fact somewhat- realising it helped the above methodology *click* into place.\n",
    "\n",
    "**Lemma:** Let $G$ be a graph with a non-trivial graph automorphism. The rows of $Z^{1:h}$ corresponding to automorphic nodes are identical.\n",
    "\n",
    "This observation leads us nicely onto the pooling step of our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 - The Sort Pool Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where the nodes don't have a natural order (i.e they're unlabeled) we use the structural insight the convolution steps have granted us to order the nodes in a consistent way before passing to the next step of the network, we call this process sort pooling.\n",
    "\n",
    "If necessary we add empty rows to $Z^{1:h}$ until it has $k$ rows, so that all every graph's $Z^{1:h}$ matrix is the same size. We order each row by right to left lexicographical ordering and call the resulting matrix $Z^{sp}$ and we've finished the sort pooling. In order to backpropagate through the convolution layers we only need to keep track of how the rows were re-ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.X - The Unfinished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is probably obvious, we intended to build and fit graph convolution networks using the three different optimisers. We didn't have the hours to do so and will have to name our old intentions 'unfinished business'. Like Clint Eastwood maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources for Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://www.kaggle.com/ang3loliveira/malware-analysis-datasets-api-call-sequences\n",
    "\n",
    "[2] Zhang, Cui, Neumann, and Chen, An End-to-End Deep Learning Architecture for Graph Classification. AAAI-18. 2018\n",
    "\n",
    "[3] Niepert, Ahmed, and Kutzkov, Learning convolutional neural networks for graphs. In Proceedings of the 33rd annual international conference on machine learning. 2016.\n",
    "\n",
    "[4] Shervashidze, Schweitzer, Leeuwen, Mehlhorn, Borgwardt. Weisfeiler-Lehman graph kernels. Journal of Machine Learning Research, 12:25392561. 2011.\n",
    "\n",
    "[5] https://towardsdatascience.com/a-loss-function-suitable-for-class-imbalanced-data-focal-loss-af1702d75d75 "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a02325cc9b61a5aff559bb1accde5bb54928b9417508f54cab82fa12c61f578"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
