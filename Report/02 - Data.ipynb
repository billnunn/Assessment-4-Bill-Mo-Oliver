{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6f0a8d-9655-48dd-86e9-97190f30d230",
   "metadata": {},
   "source": [
    "# 02 - Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657015c9-f91f-413a-b2fa-0cb83de5cdbe",
   "metadata": {},
   "source": [
    "This section explores the KDD Cup 1999 dataset briefly and acts as the setup for our subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176cce65-9af5-4dcf-933a-317a79b234fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing and Installing Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97506d95-43b6-418c-b23f-ba0466a3f126",
   "metadata": {},
   "source": [
    "To install the required libraries, uncomment the cell below and run on a blank environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d10daf-95f3-4e9e-bdde-8b4113fc24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86fee258-7107-44b8-ae51-6e0590b8eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6fb746-0ea5-4687-8fb0-6e551ba4e57c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Discussing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbab981-888b-4f14-be91-9bc69ccc5bd7",
   "metadata": {},
   "source": [
    "For this assessment, we are using a pre-processed version of the KDD Cup 1999 dataset found [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)[1], and in particular we are using a pre-processed version of the 10% subset. The pre-processing was done by the author in a previous assessment and the details of this can be found [on this GitHub repository](https://github.com/billnunn/Assessment-1-Bill-Mo)[2] under [01 - Data Setup](https://github.com/billnunn/Assessment-1-Bill-Mo/blob/main/Report/01%20-%20Data%20Setup.ipynb) in the 'Report' folder.\n",
    "\n",
    "It is worth noting that the classification task that we are going to carry out in this assessment was also caried out using RandomForests in the previous assessment with reasonable success. So, while a Neural Net may not be necessary for this particular dataset, it is still appropriate for the purpose of assessing the performance and convergence rate for different optimizer choices in this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111cdabf-4e2e-4903-b92b-f910bfe83825",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00c806-d2c6-452f-9059-a174985e937c",
   "metadata": {},
   "source": [
    "Uncomment below to download data from Google Drive. If this doesn't work please try to download manually from [here](https://drive.google.com/file/d/1XSs3-GjPwB0FAYtGVNiwdKdiJ916BI75/view?usp=sharing) and save as '../data/kdd_log_df.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba7a568-8ca7-403f-9cdb-f0ad7c7f4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0c9295-3c16-46d5-bd2f-c92d6b0bf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdd.download_file_from_google_drive(file_id='1XSs3-GjPwB0FAYtGVNiwdKdiJ916BI75',\n",
    "                                    # dest_path='../data/kdd_log_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f83bb6f-6c24-4fc9-ab2f-89809cdbc407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>num_root</th>\n",
       "      <th>num_file_creations</th>\n",
       "      <th>num_access_files</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "      <th>connection_type</th>\n",
       "      <th>connection_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.204007</td>\n",
       "      <td>8.603554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.480639</td>\n",
       "      <td>6.188264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.463832</td>\n",
       "      <td>7.198931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>7.198931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.384495</td>\n",
       "      <td>7.617268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  src_bytes  dst_bytes  urgent  hot  num_failed_logins  \\\n",
       "0       0.0   5.204007   8.603554     0.0  0.0                0.0   \n",
       "1       0.0   5.480639   6.188264     0.0  0.0                0.0   \n",
       "2       0.0   5.463832   7.198931     0.0  0.0                0.0   \n",
       "3       0.0   5.393628   7.198931     0.0  0.0                0.0   \n",
       "4       0.0   5.384495   7.617268     0.0  0.0                0.0   \n",
       "\n",
       "   num_compromised  num_root  num_file_creations  num_access_files  ...  \\\n",
       "0              0.0       0.0                 0.0               0.0  ...   \n",
       "1              0.0       0.0                 0.0               0.0  ...   \n",
       "2              0.0       0.0                 0.0               0.0  ...   \n",
       "3              0.0       0.0                 0.0               0.0  ...   \n",
       "4              0.0       0.0                 0.0               0.0  ...   \n",
       "\n",
       "   flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  flag_SF  \\\n",
       "0          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "1          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "2          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "3          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "4          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "   flag_SH  connection_type  connection_category  \n",
       "0      0.0           normal               normal  \n",
       "1      0.0           normal               normal  \n",
       "2      0.0           normal               normal  \n",
       "3      0.0           normal               normal  \n",
       "4      0.0           normal               normal  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/kdd_log_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221c050-8d27-4764-9b82-db4148e1bdbb",
   "metadata": {},
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d6805-3e96-45e9-afc1-345cc143f223",
   "metadata": {},
   "source": [
    "Examine the labels for our dataset. We have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360514c9-35a7-45f4-aaa9-fb543a93290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 23 connection types and 5 connection categories\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "connection_type  connection_category\n",
       "smurf            dos                    280790\n",
       "neptune          dos                    107201\n",
       "normal           normal                  97278\n",
       "back             dos                      2203\n",
       "satan            probe                    1589\n",
       "ipsweep          probe                    1247\n",
       "portsweep        probe                    1040\n",
       "warezclient      r2l                      1020\n",
       "teardrop         dos                       979\n",
       "pod              dos                       264\n",
       "nmap             probe                     231\n",
       "guess_passwd     r2l                        53\n",
       "buffer_overflow  u2r                        30\n",
       "land             dos                        21\n",
       "warezmaster      r2l                        20\n",
       "imap             r2l                        12\n",
       "rootkit          u2r                        10\n",
       "loadmodule       u2r                         9\n",
       "ftp_write        r2l                         8\n",
       "multihop         r2l                         7\n",
       "phf              r2l                         4\n",
       "perl             u2r                         3\n",
       "spy              r2l                         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_type = df.connection_type.nunique()\n",
    "num_cat = df.connection_category.nunique()\n",
    "print('Total of {} connection types and {} connection categories'.format(num_type,num_cat))\n",
    "print()\n",
    "df[['connection_type','connection_category']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20068e-4d1b-457c-a3a1-23fbdf737dd7",
   "metadata": {},
   "source": [
    "One of the issues of the dataset can be seen here, namely that there are fewer normal connections than there are attacks, whereas in a normal cybersecurity setting, we would expect to see the opposite; number of normal connections being orders of magnitude more than attacks.\n",
    "\n",
    "We also have an overall class imbalance when looking at each connection category as a class as seen below. This is not an issue in the cybersecurity sense, as we do expect some attacks to be more prevalent than others, but it is something to keep in mind when choosing a metric for assessing model/optimizer performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f6932d-0825-4bf7-8ee8-bf3927f9e7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.93020742033234"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(100*df.connection_category.value_counts()[:2])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac6935f-0a4a-4500-b135-b9b0fd76633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dos       391458\n",
      "normal     97278\n",
      "probe       4107\n",
      "r2l         1126\n",
      "u2r           52\n",
      "Name: connection_category, dtype: int64\n",
      "\n",
      "dos       79.239142\n",
      "normal    19.691066\n",
      "probe      0.831341\n",
      "r2l        0.227926\n",
      "u2r        0.010526\n",
      "Name: connection_category, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df.connection_category.value_counts())\n",
    "print()\n",
    "print(100*df.connection_category.value_counts()/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2181d18-04ce-4ab3-bc73-5488b5d521de",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a160ce-3733-42a4-bb90-8c0fddf4027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 115 columns are our features\n",
    "features = df[df.columns[:-2]]\n",
    "#last two columns are connection category and connection type\n",
    "targets = df[df.columns[-2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320083b1-5728-486c-9f0f-7b23ff7592a1",
   "metadata": {},
   "source": [
    "In the following, we create our training and test dataset split but we do something unusual: we assign 50% of our dataset for training. There are a few reasons for this:\n",
    "- The class imbalances are such that if we only assigned 10% or 20% of the data to training, we would only have 5 or 10 u2r connections, respectively, so assessing performance vs randomness on these becomes increasingly hard.\n",
    "- Given that RandomForests perform so well, it reasonable to expect that Neural Networks would perform as well (if not better - and indeed we found that given a larger dataset our performance was too good to allow meaningful comparison in some cases), and so we try to artificially make the problem more difficult for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461dd50-30ec-46fd-9b9f-e086046d8e83",
   "metadata": {},
   "source": [
    "Concretely, we are placing ourselves in the scenario where our available data is very limited and assessing how we perform given that we get large out-of-sample data to predict. We still stratify our data to make sure that we do not lose connection categories in one dataset due to pure randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d96e75f8-3f93-4ec2-b65a-e080faf7eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets,\n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state = 42, shuffle = True, \n",
    "                                                    stratify = targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216ff2e-eb67-40bd-b320-6779bc47ced3",
   "metadata": {},
   "source": [
    "Brief sanity check to see that our data has been shuffle and that our labels for X and y match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1349659-3752-4d86-9f20-07cc8c00ce3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>num_root</th>\n",
       "      <th>num_file_creations</th>\n",
       "      <th>num_access_files</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_REJ</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360984</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112306</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404506</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.255750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222435</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.940222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234492</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.940222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration  src_bytes  dst_bytes  urgent  hot  num_failed_logins  \\\n",
       "360984       0.0   0.000000        0.0     0.0  0.0                0.0   \n",
       "112306       0.0   0.000000        0.0     0.0  0.0                0.0   \n",
       "404506       0.0   6.255750        0.0     0.0  0.0                0.0   \n",
       "222435       0.0   6.940222        0.0     0.0  0.0                0.0   \n",
       "234492       0.0   6.940222        0.0     0.0  0.0                0.0   \n",
       "\n",
       "        num_compromised  num_root  num_file_creations  num_access_files  ...  \\\n",
       "360984              0.0       0.0                 0.0               0.0  ...   \n",
       "112306              0.0       0.0                 0.0               0.0  ...   \n",
       "404506              0.0       0.0                 0.0               0.0  ...   \n",
       "222435              0.0       0.0                 0.0               0.0  ...   \n",
       "234492              0.0       0.0                 0.0               0.0  ...   \n",
       "\n",
       "        flag_REJ  flag_RSTO  flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  \\\n",
       "360984       0.0        0.0          0.0        0.0      1.0      0.0   \n",
       "112306       0.0        0.0          0.0        0.0      1.0      0.0   \n",
       "404506       0.0        0.0          0.0        0.0      0.0      0.0   \n",
       "222435       0.0        0.0          0.0        0.0      0.0      0.0   \n",
       "234492       0.0        0.0          0.0        0.0      0.0      0.0   \n",
       "\n",
       "        flag_S2  flag_S3  flag_SF  flag_SH  \n",
       "360984      0.0      0.0      0.0      0.0  \n",
       "112306      0.0      0.0      0.0      0.0  \n",
       "404506      0.0      0.0      1.0      0.0  \n",
       "222435      0.0      0.0      1.0      0.0  \n",
       "234492      0.0      0.0      1.0      0.0  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd94db0c-c38c-44d2-99a0-e8d3327790a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>connection_type</th>\n",
       "      <th>connection_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360984</th>\n",
       "      <td>neptune</td>\n",
       "      <td>dos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112306</th>\n",
       "      <td>neptune</td>\n",
       "      <td>dos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404506</th>\n",
       "      <td>smurf</td>\n",
       "      <td>dos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222435</th>\n",
       "      <td>smurf</td>\n",
       "      <td>dos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234492</th>\n",
       "      <td>smurf</td>\n",
       "      <td>dos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       connection_type connection_category\n",
       "360984         neptune                 dos\n",
       "112306         neptune                 dos\n",
       "404506           smurf                 dos\n",
       "222435           smurf                 dos\n",
       "234492           smurf                 dos"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34d07d66-fb3e-42d4-a9a8-db31e3cf8a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 23)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.connection_type.nunique(), y_test.connection_type.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd300e-34d8-415b-856f-16e84ed0b6d2",
   "metadata": {},
   "source": [
    "And our y's have been stratified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8046ea51-722f-4dfe-8fb1-0df78c2dc8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index : smurf dos has difference: 0\n",
      "Index : neptune dos has difference: -1\n",
      "Index : normal normal has difference: 0\n",
      "Index : back dos has difference: -1\n",
      "Index : satan probe has difference: -1\n",
      "Index : ipsweep probe has difference: -1\n",
      "Index : portsweep probe has difference: 0\n",
      "Index : warezclient r2l has difference: 0\n",
      "Index : teardrop dos has difference: -1\n",
      "Index : pod dos has difference: 0\n",
      "Index : nmap probe has difference: -1\n",
      "Index : guess_passwd r2l has difference: 1\n",
      "Index : buffer_overflow u2r has difference: 0\n",
      "Index : land dos has difference: 1\n",
      "Index : warezmaster r2l has difference: 0\n",
      "Index : imap r2l has difference: 0\n",
      "Index : rootkit u2r has difference: 0\n",
      "Index : loadmodule u2r has difference: 1\n",
      "Index : ftp_write r2l has difference: 0\n",
      "Index : multihop r2l has difference: 1\n",
      "Index : perl u2r has difference: 1\n",
      "Index : phf r2l has difference: 0\n",
      "Index : spy r2l has difference: 0\n"
     ]
    }
   ],
   "source": [
    "differences = [(ind, y_train.value_counts()[ind] - y_test.value_counts()[ind]) for ind in y_train.value_counts().index]\n",
    "\n",
    "for a in differences:\n",
    "    print('Index :', a[0][0], a[0][1], 'has difference:', a[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46d435-335a-4c39-b7b2-98178f6ddcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b741ca0-766f-4ec0-861a-9ea6d8589f9e",
   "metadata": {},
   "source": [
    "Here, we look below to see a certain problem arise: one of our connection types (spy) only occurs once in our training dataset. This means that it's not possible to split into a train/validation set in any manner such that both sets contain at least 1 spy attack. Also, as seen from above, since we only had 2 spy attacks in the whole dataset, we could not perform the train/test split in such a way that we would have more spy attacks to share between train/validation.\n",
    "\n",
    "For this purpose, we perform our split stratifying for connection category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2749c6a-acb3-49fa-a743-1235cbed4ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "connection_type  connection_category\n",
       "smurf            dos                    140395\n",
       "neptune          dos                     53600\n",
       "normal           normal                  48639\n",
       "back             dos                      1101\n",
       "satan            probe                     794\n",
       "ipsweep          probe                     623\n",
       "portsweep        probe                     520\n",
       "warezclient      r2l                       510\n",
       "teardrop         dos                       489\n",
       "pod              dos                       132\n",
       "nmap             probe                     115\n",
       "guess_passwd     r2l                        27\n",
       "buffer_overflow  u2r                        15\n",
       "land             dos                        11\n",
       "warezmaster      r2l                        10\n",
       "imap             r2l                         6\n",
       "rootkit          u2r                         5\n",
       "loadmodule       u2r                         5\n",
       "ftp_write        r2l                         4\n",
       "multihop         r2l                         4\n",
       "perl             u2r                         2\n",
       "phf              r2l                         2\n",
       "spy              r2l                         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3680da4-7dea-42fa-88ae-1f48750cce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we take only 20% of the data (equivalent to 10% of whole data) for training\n",
    "#this is to artificially make the problem more difficult for our model as stated previously\n",
    "X_train_small, X_val, y_train_small, y_val = train_test_split(X_train, y_train, \n",
    "                                                             test_size = 0.8,\n",
    "                                                             random_state = 42, shuffle = True,\n",
    "                                                             stratify = y_train.connection_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa78eb6c-15df-4990-a7bc-f98556a6dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn target into binary with 0 = normal, 1 = attack\n",
    "y_train_bin = y_train_small.connection_category.apply(lambda x: 0 if x == 'normal' else 1)\n",
    "y_val_bin = y_val.connection_category.apply(lambda x: 0 if x == 'normal' else 1)\n",
    "y_test_bin = y_test.connection_category.apply(lambda x: 0 if x == 'normal' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09129596-1360-4dc7-95e5-e9504daa4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create multi category classifier variable e.g. normal,u2r,dos... \n",
    "y_train_multi = y_train_small.connection_category\n",
    "y_val_multi = y_val.connection_category\n",
    "y_test_multi = y_test.connection_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2695ec59-cf17-46c7-a316-8dad7aaaccf7",
   "metadata": {},
   "source": [
    "We do not take further steps for our categorical y's because the way they get saved and reloaded into the other notebooks causes issues with how TensorFlow likes to take in categorical data. Therefore, we process categorical y's in the models' notebooks instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d36c2-fcec-46c0-8f2b-e97240f35bfa",
   "metadata": {},
   "source": [
    "We now save our variables to a data folder so that we have the same data split to load from across all notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13b8d7d9-45a3-401a-99e4-c79803a9044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(X_train_small, '..\\data\\X_train.csv', index=False)\n",
    "pd.DataFrame.to_csv(X_val, '..\\data\\X_val.csv', index=False)\n",
    "pd.DataFrame.to_csv(X_test, '..\\data\\X_test.csv', index=False)\n",
    "\n",
    "pd.DataFrame.to_csv(y_train_bin, '..\\data\\y_train_bin.csv', index=False)\n",
    "pd.DataFrame.to_csv(y_val_bin, '..\\data\\y_val_bin.csv', index=False)\n",
    "pd.DataFrame.to_csv(y_test_bin, '..\\data\\y_test_bin.csv', index=False)\n",
    "\n",
    "pd.DataFrame(y_train_multi).to_csv('..\\data\\y_train_multi.csv', index=False)\n",
    "pd.DataFrame(y_val_multi).to_csv('..\\data\\y_val_multi.csv', index=False)\n",
    "pd.DataFrame(y_test_multi).to_csv('..\\data\\y_test_multi.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dfa871-eff1-43c2-9a64-cbec84ec27fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d2788-77d5-4891-bb95-de4fd003b668",
   "metadata": {},
   "source": [
    "Code below is for connection_type classification but that would need more tweaking when creating a neural network's architecture. The neural network would have to output some form of distance rather than probability of a certain type of attack, in case some attack type was not seen in training but was seen in test.\n",
    "\n",
    "For example, given the way that we split the data, we have `rootkit`, `ftp_write`, `warezmaster`,`multihop`,`spy`,`phf` attacks that exist either in the training or test data, but not in both (you can see this for yourself by un-commenting the two code cells below and running tehm). These existing in the training data alone is less of an issue but if they only exist in the test data then we may get nonesense classification of a class that we have not seen. One solution to this may be to stratify the data but as seen above when highlighting different types of attacks that exist, attacks like `phf`,`perl`, and `spy` appear less than 5 times each and so even a stratified sample won't be very representative if we are taking a 1:9 test:train split. Some selection process can be used wherein the training data has the first occurence of each connection type and then added on top of a stratified sample. That way the training data will always consist of a 90% + 23 datapoints which will always contain all connection types. This, however, assumes that our dataset is inclusive of all possible attack types, and introducing new data with new attacks would mean our model may not generalise very well.\n",
    "\n",
    "A model selection approach to resolve this would be to create a neural network that outputs some sort of distance from a class rather than a probability of belonging to a class, then assigning a new observation as an `other` class if its distance from all classes exceeds a certian threshold. This, however, feels like it would fall outside the scope of this assessment, and so we do not attempt it, unless we find ourselves with enough time to try this implementation.\n",
    "\n",
    "We started working on the code in a previous iteration of this file (which is why some of the variables have different names) but this was abandoned quickly for the reasons mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21fc5d62-afb5-4c5c-b807-8e6c0dea7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create multiclass category for all connection types 'normal','buffer_overflow','perl'...\n",
    "# y_train_allclass = train_data.connection_type.copy()\n",
    "# y_test_allclass = test_data.connection_type.copy()\n",
    "# #as above, factorize then turn into tf.keras categorical\n",
    "# y_train_allclass, train_classes = pd.factorize(y_train_allclass)\n",
    "# y_test_allclass, test_classes = pd.factorize(y_test_allclass)\n",
    "# #turn into tf.keras categorical\n",
    "# y_train_allclass = to_categorical(y_train_allclass, num_classes = len(train_classes))\n",
    "# y_test_allclass = to_categorical(y_test_allclass, num_classes = len(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04b5a7f5-59fe-4d03-8c30-c170ccb64336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_classes = set(list(train_classes)+list(test_classes))\n",
    "# inc_classes = [c for c in train_classes if c in test_classes]\n",
    "# exc_classes = [c for c in all_classes if c not in inc_classes]\n",
    "\n",
    "# exc_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad080d-01d6-47e4-82fc-f847dc1e1d2b",
   "metadata": {},
   "source": [
    "## References for Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c81a9-6104-43b6-a7e9-94403775b370",
   "metadata": {},
   "source": [
    "- [1] KDD Cup 1999 http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "- [2] Assessment 1 https://github.com/billnunn/Assessment-1-Bill-Mo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assessment4env",
   "language": "python",
   "name": "assessment4env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
