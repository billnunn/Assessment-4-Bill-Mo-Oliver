{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dca2288-2fca-40e0-ae16-6dfe45e8b3da",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbd0c9-d9c8-4138-baad-7fa72d169f8a",
   "metadata": {},
   "source": [
    "In this report, we examine the behaviour of different optimisers in Deep Learning. Particularly, we use standard Stochastic Gradient Descent (SGD) as a baseline and compare it to the performance of Nesterov's Accelerated Gradient (NAG), and ADAM. Concretely, we want to explore\n",
    "which minimiser converges more quickly with respect to number of training epochs, other convergence characteristics, and classification performance. \n",
    "\n",
    "We start off by comparing them on Deep Neural Nets running on KDD Cup 1999 data. We consider both the case of binary and multiclass classification. We investigate various potential DNN architectures for an SGD optimiser and hypothesise that NAG and ADAM would perform better than SGD, given the best case scenario for an SGD. In the final section we investigate binary classification for the different optimisers on an API call sequence dataset and discuss architectures designed to deal with sequence data and graph data. \n",
    "\n",
    "The relevant datasets are linked and referenced in the appropriate sections of this report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assessment4env",
   "language": "python",
   "name": "assessment4env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
