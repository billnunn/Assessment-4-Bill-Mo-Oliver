{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd01e59-581e-4a7c-9646-8978f305e2a6",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e10f15-3b9b-4157-b201-6efd002c3d03",
   "metadata": {},
   "source": [
    "For the KDD Cup 1999 dataset, we found that a DNN with a depth of 3 hidden layers performed best, whilst a width of 3 and above gave highly similar results. Because of this, we perceded with the initial architecture proposed in the SGD section. Given this setting, we found that standard SGD performed the worst out of all optimisers in terms of convergence. NAG expectedly performed better than standard SGD, though SGD with momentum seemed to converge very closely to NAG and then attain a lower minimum. ADAM achieved the lowest of all minima, at a clearly faster rate (almost immediate).\n",
    "\n",
    "However, it seemed that all the optimisers except for NAG were overfitting. This is likely due to our choice of loss function given the massive class imbalance in our dataset. Concretely, by assigning a higher 'probability' that an observation was a 'dos' or 'normal' connection, a lower loss can be attained, whilst ignoring assignment to the infrequent classes. This meant whilst NAG predicted almost all of the 'normal' and 'dos' connections, and most of the 'probe', it attained a higher loss than ADAM which predicted 'normal' and 'dos' with higher certainty, but predicted no 'probe' at all. A possible solution would have been to look at custom loss function which betters accounts for the huge class imbalance, however this was outside the scope of this report and so we did not implement this.\n",
    "\n",
    "On the other hand, in the RNN setting with the API call sequence dataset we found somewhat contradictory results with SGD and NAG critically failing and predicted only one class. This could be due to the gradient of the loss function for the dataset not being smooth enough and thus SGD and NAG struggle to converge to a as good a minima as ADAM. Our intuitive explanation is that the true minima exists near a very steep gradient such that NAG and SGD wtih momentum miss or 'jump over' the minima, whereas the adaptive stepsize of ADAM allows for convergence to that minima. Alternatively, SGD and NAG may have gotten stuck in a different local minima.\n",
    "\n",
    "While in the KDD dataset, NAG seemed to be a better optimiser, ADAM was more reliable in the API calls sequence dataset. This may have been due to ADAM converging very quickly and us not implementing an early stopping time (though seeing one on a learning curve plot was difficult), and that may have caused the overfitting. Implementing this, editing dropout rate, and/or changing batch sizes may have caused different results where ADAM performed better (and indeed for the multiclass case with no dropout and a batch size of 1000, it does) but we did not explore this. It seems that Nesterov was able to avoid overfitting after being trained for an arbitrary number of epochs with no early stopping, this is possibly due to the nature of the loss function and the KDD dataset being 'cleaner' and 'nicer' (see [Assessment 1](github.com)] where a small number of features contained a large percentage of variability in the dataset. However, ADAM seemed to be more consistent and reliable for the API calls dataset where the setting was less 'nice' or 'clean'. Further exploration into these datasets and others may shed more light on this, but that is largely outside of the scope of this assessment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assessment4env",
   "language": "python",
   "name": "assessment4env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
